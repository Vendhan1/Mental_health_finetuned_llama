🧠 Fine-Tuned Mental Health LLM
  -->A fine-tuned LLaMA 3.2B model for mental health conversations.

📚 Project Overview
  -->This project presents a custom fine-tuned Large Language Model (LLaMA 3.2B) designed to assist in mental health conversations and assessments. The model was trained using synthetic data generated from mental health resources derived from a PDF-based learning material.

🔍 Key Highlights
  -->🦙 Base Model: LLaMA 3.2B

  -->📝 Fine-Tuning Data: Synthetic conversations and Q&A derived from a curated mental health PDF.

  -->🧩 Purpose: To enable empathetic, informative, and supportive mental health interactions.

  -->🤖 Training Strategy: Used synthetic data generation to preserve privacy and avoid direct exposure to sensitive real-world data.

🚀 Features
  -->💬 Conversational Mental Health Support

  -->🧠 Knowledge-grounded Responses from Mental Health PDF Material

  -->🔒 Trained on Synthetic Data for Ethical and Safe Usage

  -->🛠️ Fine-Tuned LLaMA 3.2B for Specialized Performance

🛠️ Technical Details
  -->Model: LLaMA 3.2B

  -->Data Generation: Synthetic data creation using mental health PDFs using docling and chunker

  -->Training Method: Supervised fine-tuning, Lora

  -->Training Hardware: (3070ti 8 vram )

