ðŸ§  Fine-Tuned Mental Health LLM
  -->A fine-tuned LLaMA 3.2B model for mental health conversations.

ðŸ“š Project Overview
  -->This project presents a custom fine-tuned Large Language Model (LLaMA 3.2B) designed to assist in mental health conversations and assessments. The model was trained using synthetic data generated from mental health resources derived from a PDF-based learning material.

ðŸ” Key Highlights
  -->ðŸ¦™ Base Model: LLaMA 3.2B

  -->ðŸ“ Fine-Tuning Data: Synthetic conversations and Q&A derived from a curated mental health PDF.

  -->ðŸ§© Purpose: To enable empathetic, informative, and supportive mental health interactions.

  -->ðŸ¤– Training Strategy: Used synthetic data generation to preserve privacy and avoid direct exposure to sensitive real-world data.

ðŸš€ Features
  -->ðŸ’¬ Conversational Mental Health Support

  -->ðŸ§  Knowledge-grounded Responses from Mental Health PDF Material

  -->ðŸ”’ Trained on Synthetic Data for Ethical and Safe Usage

  -->ðŸ› ï¸ Fine-Tuned LLaMA 3.2B for Specialized Performance

ðŸ› ï¸ Technical Details
  -->Model: LLaMA 3.2B

  -->Data Generation: Synthetic data creation using mental health PDFs using docling and chunker

  -->Training Method: Supervised fine-tuning, Lora

  -->Training Hardware: (3070ti 8 vram )

